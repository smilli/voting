{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from pmf import PoissonMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "notes = pd.read_csv(os.path.join(data_path, 'notes-00000.tsv'), sep='\\t')\n",
    "# Convert NaN to empty string\n",
    "notes['summary'] = notes['summary'].astype(str).fillna('').str.strip()\n",
    "\n",
    "ratings = pd.read_csv(os.path.join(data_path, 'ratings-00000.tsv'), sep='\\t')\n",
    "\n",
    "# Drop rows with NaN in helpfulnessLevel column\n",
    "ratings = ratings.dropna(subset=['helpfulnessLevel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of ratings\n",
    "print('Total number of ratings: {}'.format(len(ratings)))\n",
    "\n",
    "# Print number of unique notes and raters\n",
    "print('Number of unique notes: {}'.format(ratings['noteId'].nunique()))\n",
    "print('Number of unique raters: {}'.format(ratings['raterParticipantId'].nunique()))\n",
    "\n",
    "# Get list of notes with more than 5 ratings\n",
    "note_rating_counts = ratings['noteId'].value_counts()\n",
    "filtered_note_ids = note_rating_counts[note_rating_counts > 5].index.tolist()\n",
    "print('Number of notes with more than 5 ratings: {}'.format(len(notes)))\n",
    "\n",
    "# Get list of raters with more than 10 ratings\n",
    "rater_counts = ratings['raterParticipantId'].value_counts()\n",
    "filtered_rater_ids = rater_counts[rater_counts > 10].index.tolist()\n",
    "print('Number of raters with more than 10 ratings: {}'.format(len(filtered_rater_ids)))\n",
    "\n",
    "# Filter ratings to only include ratings rated by raters with more than 10 ratings and for notes with more than 5 ratings\n",
    "ratings = ratings[ratings['raterParticipantId'].isin(filtered_rater_ids) & ratings['noteId'].isin(filtered_note_ids)]\n",
    "print('Number of ratings after filtering: {}'.format(len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ratings matrix to three lists:\n",
    "# - rating_labels, which is the 'helpfulnessLevel' column mapped to -1 for 'NOT_HELPFUL',\n",
    "#   0 for 'SOMEWHAT_HELPFUL', and 1 for 'HELPFUL'\n",
    "# - user_idxs, which is the 'raterParticipantId' column mapped to a unique integer\n",
    "# - note_idxs, which is the 'noteId' column mapped to a unique integer\n",
    "rating_labels = ratings['helpfulnessLevel'].map({'NOT_HELPFUL': -1, 'SOMEWHAT_HELPFUL': 0, 'HELPFUL': 1})\n",
    "# Use a label encoder to map the user and note ids to unique integers\n",
    "user_encoder = LabelEncoder()\n",
    "note_encoder = LabelEncoder()\n",
    "user_idxs = user_encoder.fit_transform(ratings['raterParticipantId'])\n",
    "note_idxs = note_encoder.fit_transform(ratings['noteId'])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_notes = len(note_encoder.classes_)\n",
    "\n",
    "# Sparse exposure matrix (did the user rate the note?)\n",
    "exp_matrix = csr_matrix((np.ones_like(rating_labels), (user_idxs, note_idxs)), shape=(n_users, n_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_notes, n_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1a: Causal Inference, Exposure Model\n",
    "Fit Poisson matrix factorization to the exposures/assignments (who rated what). We will then use the reconstructed exposures as substitute confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PoissonMF(n_components=1, random_state=42, verbose=True, a=0.3, b=0.3, c=0.3, d=0.3)\n",
    "pf.fit(exp_matrix, user_idxs, note_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent representations learned by Poisson MF\n",
    "exp_user_factors, exp_item_factors = pf.Eb, pf.Et.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1b: Causal Inference, Outcome Model\n",
    "Now estimate the outcome model, i.e., matrix factorization on the observed ratings while controlling for the substitute confounders estimated from Step 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from mf import MatrixFactorizationModel, ModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_model = MatrixFactorizationModel(n_users, n_notes, exp_user_factors=exp_user_factors, exp_item_factors=exp_item_factors, n_components=1)\n",
    "\n",
    "rating_tensor = torch.FloatTensor(rating_labels).to(mf_model.device)\n",
    "user_idxs_tensor = torch.LongTensor(user_idxs).to(mf_model.device)\n",
    "note_idxs_tensor = torch.LongTensor(note_idxs).to(mf_model.device)\n",
    "exp_tensor = torch.ones_like(rating_tensor).to(mf_model.device)\n",
    "\n",
    "data = ModelData(rating_tensor, user_idxs_tensor, note_idxs_tensor, exp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = mf_model.fit(data, epochs=100, lr=0.1, print_interval=20, validate_fraction=0.1, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Voting Aggregation\n",
    "Calculate results for different voting aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_notes = notes[notes['noteId'].isin(filtered_note_ids)]\n",
    "note_ids = note_encoder.inverse_transform(np.arange(n_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_votes = mf_model.forward_majority_vote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': majority_votes})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes[['noteId', 'summary', 'noteScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = partial(torch.quantile, q=0.25)\n",
    "quantile_scores = mf_model.get_vote_scores(quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': quantile_scores})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes[['noteId', 'summary', 'noteScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min2(x, dim):\n",
    "  return torch.min(x, dim=dim).values\n",
    "\n",
    "min_scores = mf_model.get_vote_scores(min2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': min_scores})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes.head(20)[['noteId', 'summary', 'noteScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max2(x, dim):\n",
    "  return torch.max(x, dim=dim).values\n",
    "\n",
    "max_scores = mf_model.get_vote_scores(max2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': max_scores})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes.head(20)[['noteId', 'summary', 'noteScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controversial (Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_scores = mf_model.get_vote_scores(torch.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': var_scores})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes.head(20)[['noteId', 'summary', 'noteScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % approval\n",
    "Rank by the % approval (where approval is defined as a score being above a particular threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approval(x, dim, threshold=0.7):\n",
    "    return (x > threshold).float().mean(dim=dim)\n",
    "\n",
    "approval_scores = mf_model.get_vote_scores(approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame({'noteId': note_ids, 'noteScore': approval_scores})\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')\n",
    "scored_notes = scored_notes.sort_values(by='noteScore', ascending=False)\n",
    "scored_notes.head(20)[['noteId', 'summary', 'noteScore']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "partisan-sorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
