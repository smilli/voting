{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from pmf import PoissonMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/scratch/sm2537/data/03_13_24'\n",
    "notes = pd.read_csv(os.path.join(data_path, 'notes-00000.tsv'), sep='\\t')\n",
    "# Convert NaN to empty string\n",
    "notes['summary'] = notes['summary'].astype(str).fillna('').str.strip()\n",
    "\n",
    "# read in ratings from 'ratings-00000.tsv' to 'ratings-00007.tsv'\n",
    "# and concatenate them into a single DataFrame\n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    filepath = os.path.join(data_path, f'ratings-0000{i}.tsv')\n",
    "    if i == 0:\n",
    "        ratings = pd.read_csv(filepath, sep='\\t')\n",
    "    else:\n",
    "        ratings = pd.concat([ratings, pd.read_csv(filepath, sep='\\t')])\n",
    "\n",
    "# Drop rows with NaN in helpfulnessLevel column\n",
    "ratings = ratings.dropna(subset=['helpfulnessLevel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_status_df = pd.read_csv(os.path.join(data_path, 'noteStatusHistory-00000.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of ratings\n",
    "print('Total number of ratings: {}'.format(len(ratings)))\n",
    "\n",
    "# Print number of unique notes and raters\n",
    "print('Number of unique notes: {}'.format(ratings['noteId'].nunique()))\n",
    "print('Number of unique raters: {}'.format(ratings['raterParticipantId'].nunique()))\n",
    "\n",
    "# Get list of notes with more than 5 ratings\n",
    "note_rating_counts = ratings['noteId'].value_counts()\n",
    "filtered_note_ids = note_rating_counts[note_rating_counts > 5].index.tolist()\n",
    "print('Number of notes with more than 5 ratings: {}'.format(len(notes)))\n",
    "\n",
    "# Get list of raters with more than 10 ratings\n",
    "rater_counts = ratings['raterParticipantId'].value_counts()\n",
    "filtered_rater_ids = rater_counts[rater_counts > 10].index.tolist()\n",
    "print('Number of raters with more than 10 ratings: {}'.format(len(filtered_rater_ids)))\n",
    "\n",
    "# Filter ratings to only include ratings rated by raters with more than 10 ratings and for notes with more than 5 ratings\n",
    "ratings = ratings[ratings['raterParticipantId'].isin(filtered_rater_ids) & ratings['noteId'].isin(filtered_note_ids)]\n",
    "print('Number of ratings after filtering: {}'.format(len(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ratings matrix to three lists:\n",
    "# - rating_labels, which is the 'helpfulnessLevel' column mapped to -1 for 'NOT_HELPFUL',\n",
    "#   0 for 'SOMEWHAT_HELPFUL', and 1 for 'HELPFUL'\n",
    "# - user_idxs, which is the 'raterParticipantId' column mapped to a unique integer\n",
    "# - note_idxs, which is the 'noteId' column mapped to a unique integer\n",
    "rating_labels = ratings['helpfulnessLevel'].map({'NOT_HELPFUL': -1, 'SOMEWHAT_HELPFUL': 0, 'HELPFUL': 1})\n",
    "# Use a label encoder to map the user and note ids to unique integers\n",
    "user_encoder = LabelEncoder()\n",
    "note_encoder = LabelEncoder()\n",
    "user_idxs = user_encoder.fit_transform(ratings['raterParticipantId'])\n",
    "note_idxs = note_encoder.fit_transform(ratings['noteId'])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_notes = len(note_encoder.classes_)\n",
    "\n",
    "# Sparse exposure matrix (did the user rate the note?)\n",
    "exp_matrix = csr_matrix((np.ones_like(rating_labels), (user_idxs, note_idxs)), shape=(n_users, n_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1a: Causal Inference, Exposure Model\n",
    "Fit Poisson matrix factorization to the exposures/assignments (who rated what). We will then use the reconstructed exposures as substitute confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf = PoissonMF(n_components=4, random_state=1, verbose=True, a=0.3, b=0.3, c=0.3, d=0.3)\n",
    "# pf.fit(exp_matrix, user_idxs, note_idxs)\n",
    "# Latent representations learned by Poisson MF\n",
    "# exp_user_factors, exp_item_factors = pf.Eb, pf.Et.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save exp_user_factors and exp_item_factors\n",
    "# np.save('exp_user_factors.npy', exp_user_factors)\n",
    "# np.save('exp_item_factors.npy', exp_item_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load exp_user_factors and exp_item_factors\n",
    "exp_user_factors = np.load('out/exp_user_factors.npy')\n",
    "exp_item_factors = np.load('out/exp_item_factors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1b: Causal Inference, Outcome Model\n",
    "Now estimate the outcome model, i.e., matrix factorization on the observed ratings while controlling for the substitute confounders estimated from Step 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from mf import MatrixFactorizationModel, ModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=4\n",
    "\n",
    "# Our full model that deconfounds with the substitute confounder from step 1a\n",
    "deconf_mf_model = MatrixFactorizationModel(\n",
    "    n_users, n_notes, \n",
    "    exp_user_factors=exp_user_factors,\n",
    "    exp_item_factors=exp_item_factors,\n",
    "    n_components=n_components)\n",
    "\n",
    "# Regular matrix factorization without deconfounding\n",
    "mf_model = MatrixFactorizationModel(n_users, n_notes, n_components=n_components)\n",
    "\n",
    "rating_tensor = torch.FloatTensor(rating_labels.values).to(deconf_mf_model.device)\n",
    "user_idxs_tensor = torch.LongTensor(user_idxs).to(deconf_mf_model.device)\n",
    "note_idxs_tensor = torch.LongTensor(note_idxs).to(deconf_mf_model.device)\n",
    "exp_tensor = torch.ones_like(rating_tensor).to(deconf_mf_model.device)\n",
    "\n",
    "data = ModelData(rating_tensor, user_idxs_tensor, note_idxs_tensor, exp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = deconf_mf_model.fit(data, epochs=150, lr=0.1, print_interval=20, validate_fraction=0.1, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = mf_model.fit(data, epochs=150, lr=0.1, print_interval=20, validate_fraction=0.1, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Voting Aggregation\n",
    "Calculate results for different voting aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aggregations\n",
    "def approval(x, dim, threshold=0.7):\n",
    "    return (x > threshold).float().mean(dim=dim)\n",
    "quantile = partial(torch.quantile, q=0.25)\n",
    "\n",
    "# Collect aggregations into dict\n",
    "filtered_notes = notes[notes['noteId'].isin(filtered_note_ids)]\n",
    "note_ids = note_encoder.inverse_transform(np.arange(n_notes))\n",
    "aggs = {'noteId': note_ids}\n",
    "\n",
    "# Aggregations with deconfounder model\n",
    "aggs['mean'] = mf_model.get_vote_scores(torch.mean)\n",
    "aggs['approval'] = mf_model.get_vote_scores(approval)\n",
    "aggs['quantile'] = mf_model.get_vote_scores(quantile)\n",
    "#aggs['var'] = mf_model.get_vote_scores(torch.var)\n",
    "\n",
    "# Aggregations with deconfounder mf model\n",
    "aggs['decon_mean'] = deconf_mf_model.get_vote_scores(torch.mean)\n",
    "aggs['decon_approval'] = deconf_mf_model.get_vote_scores(approval)\n",
    "aggs['decon_quantile'] = deconf_mf_model.get_vote_scores(quantile)\n",
    "#aggs['var'] = deconf_mf_model.get_vote_scores(torch.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {k: v.cpu().numpy() for k, v in aggs.items() if k != 'noteId'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs['noteId'] = note_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_results = pd.DataFrame(aggs)\n",
    "scored_notes = filtered_notes.merge(note_results, on='noteId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "As a first-pass for evaluation, compare the models (with and without causal inference) in how well they agree with the current Community Notes algorithm. Obviously, eventually we would like to show that we do better than the existing algorithm in some way, so we will need different evaluations down the line, but this is just a quick first pass.\n",
    "\n",
    "The deconfounded model (that uses causal inference in stage 1) does significantly better in matching the existing algorithm's outputs than a baseline of matrix factorization + voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_status_df = pd.read_csv(os.path.join(data_path, 'noteStatusHistory-00000.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_notes = scored_notes.merge(note_status_df, on='noteId')\n",
    "misleading_notes = merged_notes[merged_notes['classification'] == 'MISINFORMED_OR_POTENTIALLY_MISLEADING']\n",
    "misleading_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_status_key = 'currentStatus'\n",
    "num_rated_helpful = misleading_notes[note_status_key].value_counts()['CURRENTLY_RATED_HELPFUL']\n",
    "print(f'Number of notes rated helpful under existing algorithm: {num_rated_helpful}')\n",
    "agg_keys = list(aggs.keys())\n",
    "agg_keys.remove('noteId')\n",
    "for key in agg_keys:\n",
    "    helpful_notes = misleading_notes.sort_values(key, ascending=False).head(num_rated_helpful)\n",
    "    num_helpful = helpful_notes[note_status_key].value_counts()['CURRENTLY_RATED_HELPFUL']\n",
    "    pct_helpful = num_helpful / num_rated_helpful\n",
    "    print(f'Percentage of CURRENTLY_RATED_HELPFUL notes in top {num_rated_helpful} notes using {key} aggregation: {pct_helpful:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the deconfounded model (that uses causal inference in stage 1) seems to do significantly better in matching the existing algorithm's outputs than a baseline of matrix factorization + voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences with existing model\n",
    "\n",
    "### Notes rated helpful under deconfounder model (ours) but not under existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpful_notes = misleading_notes.sort_values('decon_mean', ascending=False).head(num_rated_helpful)\n",
    "diff_notes = helpful_notes[helpful_notes[note_status_key] != 'CURRENTLY_RATED_HELPFUL']\n",
    "diff_notes[['noteId', 'tweetId', 'summary', 'currentStatus', 'currentCoreStatus', 'mostRecentNonNMRStatus', 'lockedStatus', 'decon_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_notes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes rated helpful under existing model but not under deconfounder model (ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_rated_helpful_by_deconf = misleading_notes.sort_values('decon_mean', ascending=False).head(num_rated_helpful)\n",
    "helpful_note_ids_deconf = notes_rated_helpful_by_deconf['noteId'].values\n",
    "notes_rated_helpful_by_existing_algo = misleading_notes[misleading_notes[note_status_key] == 'CURRENTLY_RATED_HELPFUL']\n",
    "diff_notes = notes_rated_helpful_by_existing_algo[~notes_rated_helpful_by_existing_algo['noteId'].isin(helpful_note_ids_deconf)]\n",
    "diff_notes = diff_notes.sort_values('decon_mean', ascending=False)\n",
    "diff_notes[['noteId', 'tweetId', 'summary', 'currentStatus', 'currentCoreStatus', 'mostRecentNonNMRStatus', 'lockedStatus', 'decon_mean']].tail(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
